# Confronto EFE: Omeostatico vs Estrinseco (TD Learning)

## Sommario della Ricerca

Questo documento riassume il confronto tra due approcci per il calcolo del **Pragmatic Value** nell'Expected Free Energy (EFE) di un agente Active Inference applicato al controllo di un termostato intelligente.

---

## 1. Background Teorico

### 1.1 Expected Free Energy (EFE)

L'Expected Free Energy √® composta da due termini:

```
EFE = Epistemic Value + Pragmatic Value
```

- **Epistemic Value**: Riduzione dell'incertezza (information gain)
- **Pragmatic Value**: Vicinanza agli obiettivi/preferenze

### 1.2 Due Approcci per il Pragmatic Value

| Approccio | Formula | Descrizione |
|-----------|---------|-------------|
| **Omeostatico** | `-(predicted_temp - target_temp)¬≤` | Costo quadratico della distanza dal target |
| **Estrinseco (TD)** | `V(s) + E[r(s,a)]` | Valore appreso + reward atteso |

---

## 2. Implementazioni

### 2.1 File Creati

| File | Descrizione |
|------|-------------|
| `smart_thermostat.py` | Agente omeostatico originale |
| `smart_thermostat_extrinsic.py` | Agente con TD Learning |
| `smart_thermostat_comparison.py` | Confronto in condizioni identiche |
| `smart_thermostat_complex_rewards.py` | Scenario con reward complessi |
| `smart_thermostat_asymmetric.py` | Scenario con reward asimmetrici |

### 2.2 Temporal Difference Learning

L'agente estrinseco implementa TD(Œª) con:

```python
# TD Error
Œ¥ = r + Œ≥ * V(s') - V(s)

# Value Update con Eligibility Traces
e(s) ‚Üê Œ≥Œª * e(s) + 1  # per stato corrente
V(s) ‚Üê V(s) + Œ± * Œ¥ * e(s)
```

**Parametri utilizzati:**
- Learning rate (Œ±): 0.1 - 0.2
- Discount factor (Œ≥): 0.9 - 0.95
- Eligibility trace (Œª): 0.8 - 0.9

---

## 3. Esperimenti e Risultati

### 3.1 Scenario 1: Reward Simmetrico Semplice

**Struttura Reward:**
- In comfort zone (¬±2¬∞C): +0.3
- Fuori comfort zone: -0.5
- Costo riscaldamento: -0.2

**Risultati (simulazione 3600s):**

| Metrica | Omeostatico | Estrinseco (TD) | Vincitore |
|---------|-------------|-----------------|-----------|
| Tempo sopravvivenza | 3601s | 3176s | üîµ Omeostatico |
| Comfort Zone | 51.5% | 44.1% | üîµ Omeostatico |
| Budget finale | ‚Ç¨58.80 | ‚Ç¨0.00 | üîµ Omeostatico |
| Rewards totali | +241.80 | +191.10 | üîµ Omeostatico |

**üèÜ Vincitore: OMEOSTATICO (6-0)**

**Motivo:** Il reward √® una funzione diretta della distanza dal target. L'omeostatico ha gi√† codificata esplicitamente questa relazione, mentre TD deve apprenderla.

---

### 3.2 Scenario 2: Reward Asimmetrico

**Struttura Reward:**
- In comfort zone (¬±2¬∞C): +0.4
- **Troppo CALDO**: Penalit√† ESPONENZIALE `base + exp(scale * excess)`
- Troppo freddo: Penalit√† lineare `linear * excess`

**Risultati:**

| Metrica | Omeostatico | Estrinseco (TD) | Vincitore |
|---------|-------------|-----------------|-----------|
| Cumulative Reward | -194.4 | -194.2 | üü† Estrinseco |
| Hot Penalty | 208.2 | 169.3 | üü† Estrinseco |
| Cold Penalty | 19.3 | 48.8 | üîµ Omeostatico |
| % Troppo Caldo | 28.8% | 24.9% | üü† Estrinseco |
| % Troppo Freddo | 35.4% | 49.0% | üîµ Omeostatico |

**üèÜ Vincitore: ESTRINSECO TD**

**Motivo:** TD ha imparato che la zona "troppo caldo" ha penalit√† esponenziali, quindi preferisce stare leggermente troppo freddo. L'omeostatico, usando `(temp - target)¬≤`, tratta le due direzioni come equivalenti.

---

## 4. Analisi Teorica

### 4.1 Quando l'Omeostatico √® Superiore

L'approccio omeostatico `(temp - target)¬≤` eccelle quando:

1. **Reward √® simmetrico**: La penalit√† per essere sopra o sotto il target √® uguale
2. **Reward √® proporzionale alla distanza**: Pi√π lontani = proporzionalmente peggio
3. **Nessuna dinamica nascosta**: Il sistema √® completamente osservabile
4. **Target noto a priori**: L'agente conosce esattamente dove vuole andare

**Vantaggi:**
- ‚úÖ Nessun warm-up necessario
- ‚úÖ Risposta immediata e precisa
- ‚úÖ Computazionalmente efficiente
- ‚úÖ Interpretabile

### 4.2 Quando TD Learning √® Superiore

L'approccio TD eccelle quando:

1. **Reward asimmetrico**: Es. troppo caldo √® peggio di troppo freddo
2. **Reward non-lineare**: Es. exponenziale, logaritmico, a gradini
3. **Reward con memoria**: Es. bonus per stabilit√†, penalit√† per oscillazioni
4. **Reward sconosciuto**: L'agente deve scoprire cosa ottimizzare
5. **Dinamiche complesse**: L'ambiente ha pattern nascosti

**Vantaggi:**
- ‚úÖ Adattabile a qualsiasi struttura di reward
- ‚úÖ Pu√≤ apprendere preferenze implicite
- ‚úÖ Cattura dipendenze temporali
- ‚úÖ Generalizza a nuove situazioni

### 4.3 Trade-off Fondamentale

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                 ‚îÇ
‚îÇ   OMEOSTATICO                         ESTRINSECO (TD)           ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ   ‚îÇ  Veloce     ‚îÇ                     ‚îÇ  Adattivo   ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  Preciso    ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ  Flessibile ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  Rigido     ‚îÇ                     ‚îÇ  Lento      ‚îÇ           ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ   Reward noto                          Reward sconosciuto       ‚îÇ
‚îÇ   Simmetrico                           Asimmetrico              ‚îÇ
‚îÇ   Statico                              Dinamico                 ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 5. Implicazioni per Active Inference

### 5.1 Connessione con la Teoria

Nella teoria dell'Active Inference:

- **Pragmatic Value omeostatico** corrisponde al concetto di **preferenze prior** fisse
- **Pragmatic Value estrinseco** corrisponde a **preferenze apprese** dall'esperienza

Entrambi sono compatibili con il framework Free Energy Principle:

```
G = E_Q[log Q(s') - log P(o|s') - log P(s')]
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     Epistemic       Instrumental    Prior
     (same)          (different!)    Preference
```

### 5.2 Biologicamente Plausibile?

| Aspetto | Omeostatico | TD Learning |
|---------|-------------|-------------|
| Plausibilit√† | Alta (riflessi innati) | Alta (apprendimento) |
| Esempio biologico | Termoregolazione | Condizionamento operante |
| Localizzazione | Ipotalamo | Gangli basali, corteccia |

---

## 6. Raccomandazioni Pratiche

### 6.1 Quando Usare Omeostatico

```
‚úÖ Obiettivo chiaro e ben definito
‚úÖ Reward semplice e simmetrico
‚úÖ Risorse computazionali limitate
‚úÖ Necessit√† di risposta immediata
```

### 6.2 Quando Usare TD Learning

```
‚úÖ Reward complesso o sconosciuto
‚úÖ Asimmetrie nelle preferenze
‚úÖ Dipendenze temporali nel reward
‚úÖ Ambiente che cambia nel tempo
```

### 6.3 Approccio Ibrido (Raccomandato)

Per applicazioni reali, si pu√≤ usare un **approccio ibrido**:

1. **Inizializzazione**: Usare costo omeostatico come baseline
2. **Apprendimento**: TD corregge gradualmente basandosi su reward reali
3. **Convergenza**: Peso shift verso TD man mano che apprende

```python
pragmatic_value = (1 - learning_progress) * homeostatic_cost + 
                  learning_progress * td_learned_value
```

---

## 7. Conclusioni

### 7.1 Sintesi dei Risultati

| Scenario | Vincitore | Margine |
|----------|-----------|---------|
| Reward simmetrico semplice | Omeostatico | 6-0 |
| Reward asimmetrico (hot>>cold) | TD Learning | Reward -194.2 vs -194.4 |

### 7.2 Takeaway Principale

> **L'approccio omeostatico `(temp - target)¬≤` √® ottimale quando la struttura del reward √® nota, simmetrica e proporzionale alla distanza.**
>
> **Il Temporal Difference Learning diventa vantaggioso quando il reward √® asimmetrico, non-lineare, dipendente dalla storia, o sconosciuto a priori.**

### 7.3 Contributo alla Tesi

Questo lavoro dimostra che:

1. L'EFE in Active Inference pu√≤ essere calcolata con approcci diversi
2. La scelta dell'approccio dipende dal dominio applicativo
3. TD Learning offre flessibilit√† per scenari complessi
4. L'approccio omeostatico rimane valido per obiettivi semplici

---

## 8. Appendice: Codice Chiave

### 8.1 EFE Omeostatico

```python
def compute_efe_homeostatic(self, action, target_temp, predicted_mean, predicted_cov):
    # Epistemic: information gain
    epistemic_value = -0.5 * np.log(predicted_cov[0, 0] + 1e-6)
    
    # Pragmatic: SYMMETRIC quadratic distance
    predicted_temp = predicted_mean[0]
    pragmatic_cost = (predicted_temp - target_temp) ** 2
    
    # EFE (minimize)
    efe = -epistemic_value + pragmatic_cost
    return efe
```

### 8.2 EFE Estrinseco (TD)

```python
def compute_efe_extrinsic(self, action, target_temp, predicted_mean, predicted_cov):
    # Epistemic: same as homeostatic
    epistemic_value = -0.5 * np.log(predicted_cov[0, 0] + 1e-6)
    
    # Pragmatic: TD-learned value + expected reward
    learned_value = self.value_function.get_value(predicted_temp)
    expected_reward = self.get_expected_reward(predicted_temp, target_temp)
    pragmatic_value = 0.5 * learned_value + 0.5 * expected_reward
    
    # EFE (minimize)
    efe = -epistemic_value - pragmatic_value
    return efe
```

### 8.3 TD Update

```python
def update_td(self, current_temp, reward, next_temp):
    current_bin = self.temp_to_bin(current_temp)
    next_bin = self.temp_to_bin(next_temp)
    
    # TD Error: Œ¥ = r + Œ≥V(s') - V(s)
    td_error = reward + self.gamma * self.V[next_bin] - self.V[current_bin]
    
    # Eligibility trace update
    self.eligibility *= self.gamma * self.lambda_
    self.eligibility[current_bin] += 1.0
    
    # Value update: V ‚Üê V + Œ±Œ¥e
    self.V += self.alpha * td_error * self.eligibility
    
    return td_error
```

---

## 9. Riferimenti

1. Friston, K. (2010). The free-energy principle: a unified brain theory?
2. Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction.
3. Parr, T., & Friston, K. J. (2019). Generalised free energy and active inference.
4. Da Costa, L., et al. (2020). Active inference on discrete state-spaces.

---

*Documento generato il 2026-01-20*
*Progetto: Smart Thermostat Active Inference*
